{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **WELCOME TO StockPredDemo_v.6.0.**"
      ],
      "metadata": {
        "id": "GGb7rK-R68GR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XURlZyX4Jw4J"
      },
      "source": [
        "# AI-aid Investment analysis WebApp v6.0\n",
        "## Ethan Aug***usto Gonzalez Bonorino***\n",
        "### Status report for v5.0\n",
        "\n",
        "- Build Deep Learning models with TensorFlow. (2 out of 4)\n",
        "- Improve front end design. ☑\n",
        "- Full implementation of Portfolio Optimization\n",
        "- plotly express has been fixed\n",
        "- Added technical indicators\n",
        "\n",
        "### Things to fix/add for v6.0\n",
        "\n",
        "- Finish models\n",
        "- Finish adding technical indicators\n",
        "- If twint is fixed merge sentiment and historical data for training\n",
        "- ALTERNATIVE: use finViz or other package to scrape financial news and use news sentiment instead of tweets.\n",
        "- Deploy WebApp for demo and sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NOTE: New sentiment model runs faster (100 tweets < 1m) but it's constrained to tweets in English. Just to consider in the report. Also, we run out of tweets to scrape after fetching 5 per day for a year and change.**"
      ],
      "metadata": {
        "id": "2wXrjUoAnr1v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi6PHK5VKLSy"
      },
      "source": [
        "## **Technologies** 📋\n",
        "\n",
        "* **Twint** -> Tweets scraper\n",
        "* **Streamlit** -> To build and share our WebApp\n",
        "* **Python** -> Programming language of choice\n",
        "* **Tensorflow** -> For optimizing and implementing models\n",
        "* **Yfinance** -> For historical and technical stock data\n",
        "* **BERT/VADER** -> To compute tweets sentiment scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCnvTIqfkA00"
      },
      "source": [
        "## Install and Import dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDDX-1UpLxak"
      },
      "source": [
        "## Twint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK5MPZ8Aj_p9",
        "outputId": "5ad96c8e-f104-47a8-de3c-ba7b279df759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'twint'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 47 (delta 3), reused 14 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "# Twint\n",
        "\n",
        "!git clone --depth=1 https://github.com/twintproject/twint.git "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open twint -> requirements.txt and specify aiohttp==3.7.0 before running the next cell"
      ],
      "metadata": {
        "id": "XYSehaXdpbu4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1u6P950GkdxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552048d0-a5c0-4bb4-84e0-a4b603328274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 35.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 263 kB 54.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 378 kB 46.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 66.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 291 kB 67.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 37.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25h  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.8.0 yfinance-0.1.70\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 21.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 48.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 164 kB 57.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 56.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 63.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 54.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 793 kB 37.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 130 kB 60.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 428 kB 58.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 381 kB 62.8 MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.13.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.33.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 23.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 60.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 880 kB 56.2 MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 462 kB 27.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.9 MB 23.1 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.19.3 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.13.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.33.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 61 kB 6.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 55.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 24.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 28.1 MB/s \n",
            "\u001b[?25h  Building wheel for pandas-ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "## must update requirements.txt for compatibility aiohttp==3.7.0\n",
        "\n",
        "!cd /content/twint && pip3 install . -r requirements.txt --quiet\n",
        "\n",
        "# Yahoo Finance API\n",
        "\n",
        "!pip install yfinance --upgrade --no-cache-dir\n",
        "\n",
        "## Pyngrok\n",
        "\n",
        "!pip install streamlit --quiet\n",
        "!pip install pyngrok==4.1.1 --quiet\n",
        "\n",
        "!pip install transformers --quiet\n",
        "\n",
        "!pip install tensorflow --quiet\n",
        "\n",
        "!pip install numpy==1.19.3 --quiet\n",
        "\n",
        "!pip install emojis --quiet\n",
        "\n",
        "!pip install PyPortfolioOpt --quiet\n",
        "\n",
        "!pip install pyyaml==5.4.1 --quiet\n",
        "\n",
        "!pip install pandas_ta --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, run the following cell."
      ],
      "metadata": {
        "id": "2AXvrTefqWpR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GdG3cSM6psz3"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pjzW98In_VWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ_5aTfSL_K0"
      },
      "source": [
        "## Streamlit WebApp design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQDj4caUgXeF",
        "outputId": "71b8cf2f-4a2d-4a6d-f952-2fe72fafe851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import twint\n",
        "import time\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "import numpy as np\n",
        "\n",
        "import pandas_datareader as pdr\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "import os\n",
        "import re\n",
        "import emojis\n",
        "from PIL import Image\n",
        "\n",
        "import streamlit as st\n",
        "import yfinance as yf\n",
        "\n",
        "# from pypfopt.expected_returns import mean_historical_return, ema_historical_return\n",
        "# from pypfopt.risk_models import CovarianceShrinkage, sample_cov, exp_cov\n",
        "# from pypfopt.efficient_frontier import EfficientFrontier\n",
        "# from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices\n",
        "# from pypfopt import HRPOpt\n",
        "# from pypfopt.efficient_frontier import EfficientCVaR\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SimpleRNN, Flatten, Dropout\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "import datetime\n",
        "\n",
        "from dateutil import parser\n",
        "\n",
        "timestr = time.strftime(\"%Y%m%d\")\n",
        "\n",
        "######## FUNCTIONS ########\n",
        "\n",
        "def get_ticker(name):\n",
        "\n",
        "    ticker = yf.Ticker(name)  \n",
        "    return ticker\n",
        "\n",
        "def get_hist_data(ticker, start, end, include_period = False, period = 'max'):\n",
        "\n",
        "    res = 0\n",
        "\n",
        "    if include_period == True:\n",
        "        ticker = yf.Ticker(ticker)\n",
        "        dataset = ticker.history(period=period)\n",
        "        res = dataset\n",
        "\n",
        "    else:\n",
        "        res = yf.download(ticker, start=start, end=end)\n",
        "\n",
        "    return pd.DataFrame(res)\n",
        "\n",
        "def compute_RSI(df, window, adjust=False):\n",
        " \n",
        "  delta = df['y']\n",
        "  loss = delta.copy()\n",
        "  gains = delta.copy()\n",
        "\n",
        "  loss[loss > 0] = 0\n",
        "  loss[loss == 0] = 1\n",
        "\n",
        "  gain_ewm = gains.ewm(com=window - 1, adjust=adjust).mean()\n",
        "  loss_ewm = abs(loss.ewm(com=window - 1, adjust=adjust).mean())\n",
        "\n",
        "  RS = gain_ewm / loss_ewm\n",
        "  RSI = 100 - 100 / (1 + RS)\n",
        "\n",
        "  return RSI\n",
        "\n",
        "def append_TI(df, tech_indicators):\n",
        "\n",
        "  for indicator in tech_indicators:\n",
        "    if indicator == \"SMA2 (rolling mean)\":\n",
        "      df[\"SMA_2\"] = df.Close.rolling(2).mean()\n",
        "\n",
        "    if indicator == \"RSI\":\n",
        "      df['RSI'] = compute_RSI(df, 14)\n",
        "    \n",
        "    if indicator == \"Force Index\":\n",
        "      df[\"Force_Index\"] = df[\"Close\"] * df[\"Volume\"]\n",
        "\n",
        "    if indicator == \"MACD\":\n",
        "      df.ta.macd(close='close', fast=12, slow=26, signal=9, append=True)\n",
        "\n",
        "    if indicator == \"Aroon Oscillator\":\n",
        "      df.ta.aroon(append = True)\n",
        "      df = df.drop([\"AROOND_14\", \"AROONU_14\"], axis = 1)\n",
        "\n",
        "    if indicator == \"On-Balance Volume (OBV)\":\n",
        "      df.ta.obv(append = True)\n",
        "\n",
        "  return df\n",
        "\n",
        "def get_recommendations(ticker):\n",
        "\n",
        "    analyst = ticker.recommendations\n",
        "\n",
        "    analyst_df = pd.DataFrame(analyst)\n",
        "\n",
        "    return analyst_df\n",
        "\n",
        "# News feed\n",
        "def print_news(ticker):\n",
        "\n",
        "    news = ticker.news\n",
        "\n",
        "    news_df = pd.DataFrame()\n",
        "    titles = []\n",
        "    publishers = []\n",
        "    links = []\n",
        "\n",
        "    for doc in news:\n",
        "\n",
        "      title = doc['title']\n",
        "      publisher = doc['publisher']\n",
        "      link = doc['link']\n",
        "\n",
        "      titles.append(title)\n",
        "      publishers.append(publisher)\n",
        "      links.append(link)\n",
        "\n",
        "    news_df['Title'] = titles\n",
        "    news_df['Publisher'] = publishers\n",
        "    news_df['Link'] = links\n",
        "\n",
        "    return news_df\n",
        "\n",
        "def plot_time_series(df):\n",
        "\n",
        "    fig = go.Figure([go.Scatter(x=df.index, y=df['Adj Close'])])\n",
        "\n",
        "    fig.update_layout(\n",
        "        title= str(ticker_name) + ' data',\n",
        "        yaxis_title= str(ticker_name) + ' Stock price')\n",
        "\n",
        "    return st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def make_candlestick(company, ticker_name):\n",
        "    fig = go.Figure(data=[go.Candlestick(\n",
        "                x=company.index,\n",
        "                open=company['Open'],\n",
        "                high=company['High'],\n",
        "                low=company['Low'],\n",
        "                close=company['Close'])])\n",
        "\n",
        "    fig.update_layout(\n",
        "        title= str(ticker_name) + ' data',\n",
        "        yaxis_title= str(ticker_name) + ' Stock price')\n",
        "    \n",
        "    return st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def make_ohlc(company, ticker_name):\n",
        "    fig = go.Figure(data=[go.Ohlc(\n",
        "                x=company.index,\n",
        "                open=company['Open'],\n",
        "                high=company['High'],\n",
        "                low=company['Low'],\n",
        "                close=company['Close'])])\n",
        "\n",
        "    fig.update_layout(\n",
        "        title= str(ticker_name) + ' data',\n",
        "        yaxis_title= str(ticker_name) + ' Stock price')\n",
        "    \n",
        "    return st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def make_efficient_frontier(x, y):\n",
        "    fig = go.Figure(data = [go.Scatter(x=x, y=y, mode = 'markers', marker=dict(\n",
        "                            size=7,\n",
        "                            color=y, #set color equal to a variable\n",
        "                            colorscale='Viridis', # one of plotly colorscales\n",
        "                            showscale=False))])\n",
        "\n",
        "    fig.update_layout(\n",
        "        title= \"Portfolio Efficient Frontier\",\n",
        "        xaxis_title = \"Risk (Volatility)\",\n",
        "        yaxis_title= \"Expected returns\")\n",
        "    \n",
        "    return st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def make_efficient_frontier_opt(x, y, min_port_x, min_port_y, max_port_x, max_port_y, min_volatility = True):\n",
        "\n",
        "    fig = go.Figure(data = [go.Scatter(x=x, y=y, mode = 'markers', marker=dict(\n",
        "                            size=7,\n",
        "                            color=y, #set color equal to a variable\n",
        "                            colorscale='Viridis', # one of plotly colorscales\n",
        "                            showscale=False))])\n",
        "\n",
        "    fig.update_layout(\n",
        "        title= \"Optimized Portfolio Allocation in Efficient Frontier\",\n",
        "        xaxis_title = \"Risk (Volatility)\",\n",
        "        yaxis_title= \"Expected returns\")\n",
        "    \n",
        "    if min_volatility == True:\n",
        "\n",
        "        fig.add_trace(go.Scatter(x=[min_port_x], y=[min_port_y],\n",
        "                        mode='markers', marker_symbol = 'star', marker = dict(color='blue', size = 13),\n",
        "                        name='Minimum Volatility Portfolio'))\n",
        "        \n",
        "    else:\n",
        "        fig.add_trace(go.Scatter(x=[max_port_x], y=[max_port_y],\n",
        "                        mode='markers', marker_symbol = 'star', marker = dict(color='red', size = 13),\n",
        "                        name='Maximum Sharpe Ratio Portfolio'))\n",
        "\n",
        "    \n",
        "    return st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def portfolio_metrics(returns, weights, rf, index='Trial'):\n",
        "    \n",
        "    '''\n",
        "    This function generates the relative performance metrics that will be reported and will be used\n",
        "    to find the optimal weights.\n",
        "    \n",
        "    Parameters:\n",
        "    weights: initialized weights or optimal weights for performance reporting\n",
        "    \n",
        "    '''   \n",
        "    \n",
        "    rp = (returns.mean()*252)@weights \n",
        "    port_var = weights@(cov*252)@weights\n",
        "    sharpe = (rp-rf)/np.sqrt(port_var)\n",
        "    df = pd.DataFrame({\"Expected Return\": rp,\n",
        "                       \"Portfolio Variance\":port_var,\n",
        "                       'Portfolio Std': np.sqrt(port_var),\n",
        "                       'Sharpe Ratio': sharpe}, index=[index])\n",
        "    return df\n",
        "\n",
        "def optimal_portfolio_weights(weights, stocks):\n",
        "    df = pd.DataFrame({\"Stock\": stocks,\n",
        "                       \"Weights\": weights})\n",
        "    df.Weights = df.Weights.map(lambda x: '{:.2%}'.format(x))\n",
        "    return df\n",
        "\n",
        "def get_tweets(ticker, num_tweets, start, end):\n",
        "\n",
        "    config = twint.Config()\n",
        "\n",
        "    config.Search = \"$\" + ticker\n",
        "    config.Lang = \"en\"\n",
        "    config.Since = start\n",
        "    config.Until = end\n",
        "    config.Limit = num_tweets\n",
        "    #config.Min_retweets = 50\n",
        "\n",
        "    config.Pandas = True\n",
        "\n",
        "    twint.run.Search(config)\n",
        "\n",
        "    tweets_df = twint.storage.panda.Tweets_df.head(num_tweets)\n",
        "\n",
        "    if len(tweets_df) < 1 or type(tweets_df['date']) == float:\n",
        "        data = pd.DataFrame([[\"None\", start]], columns=['tweet', 'date'])\n",
        "    else:\n",
        "        data = pd.DataFrame(tweets_df[['tweet', 'date']])\n",
        "\n",
        "    return data\n",
        "\n",
        "def tweetdf_to_list(tweet_df):\n",
        "    tweets = [line for line in tweet_df['tweet']]\n",
        "\n",
        "    return tweets\n",
        "\n",
        "def tweetDates_to_DateTime(df):\n",
        "  i = 0\n",
        "  for column in df['Date']:\n",
        "    dt_obj = datetime.datetime.strptime(column, '%Y-%m-%d %H:%M:%S')\n",
        "    i = i + 1\n",
        "\n",
        "  return df\n",
        "\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "def compute_sentiment(model_name):\n",
        " \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    return classifier\n",
        "\n",
        "def convert_ISO8601(df):\n",
        "  i = 0\n",
        "  for column in df['Date']:\n",
        "    date = pd.to_datetime(column)\n",
        "    date = date.date()\n",
        "    df['Date'][i] = date\n",
        "    i = i + 1\n",
        "  \n",
        "  return df\n",
        "\n",
        "def correlation_matrix(df):\n",
        "\n",
        "  df_corr = df.corr() # Generate correlation matrix\n",
        "\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(\n",
        "      go.Heatmap(\n",
        "          x = df_corr.columns,\n",
        "          y = df_corr.index,\n",
        "          z = np.array(df_corr)\n",
        "      )\n",
        "  )\n",
        "\n",
        "  return st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "######## MAIN BODY ########\n",
        "\n",
        "st.title('AI-aid investment analysis tool')\n",
        "st.subheader(\"by Augusto Gonzalez-Bonorino and Ethan Aug\")\n",
        "\n",
        "# Containers help separate content. \n",
        "# Make sure to assign relevant names to distinguish containers\n",
        "description = st.container()\n",
        "description.write(\"\"\"The objective of this Webapp is to provide informative data and models of the stock market while empowering the user.\n",
        "                    This app provides a lot of flexibility in terms of input: Choose your own ticker and time range to investigate and gain insight\n",
        "                    on key metrics. With the everincreasing influence of technology, we felt it was important to include a social media sentiment score.\n",
        "                    Scraping Twitter in real time, the app outputs tweets containing your selected ticker, calculates a daily sentiment score and uses\n",
        "                    this alongside historical data in the models. There is also flexibility in model selection and hyperparameter tuning. \n",
        "                    Investigate different stocks, dates, models and hyperparamters to come to a data driven solution. Power to the user. \"\"\")\n",
        "\n",
        "######### SIDEBAR MENU ########\n",
        "st.sidebar.header(\"Data Science Capping \\n Financial Engineering\")\n",
        "\n",
        "######### MAIN BODY ##########\n",
        "ticker_name = st.text_input(\"Enter ticker to analyze\")\n",
        "\n",
        "ticker = get_ticker(ticker_name)\n",
        "\n",
        "st.text('Specify range of time:')\n",
        "\n",
        "today = datetime.date.today()\n",
        "tomorrow = today + datetime.timedelta(days=1)\n",
        "\n",
        "start_date = st.date_input('Start date', today)\n",
        "end_date = st.date_input('End date', tomorrow)\n",
        "\n",
        "if len(ticker_name) >= 1: \n",
        "  stock_prices = get_hist_data(ticker_name, start_date, end_date)\n",
        "\n",
        "\n",
        "######## COMPANY SUMMARY ########\n",
        "\n",
        "# detailed summary on Ticker\n",
        "if st.checkbox('Show company summary'):\n",
        "\n",
        "    st.subheader('Company summary on: ' + str(ticker.info['longName']))\n",
        "    st.write(ticker.info['longBusinessSummary']) \n",
        "\n",
        "######## RAW DATA DATAFRAME ########\n",
        "\n",
        "## Make options inside checkbox bubbles a dropdown menu instead of more checkbox objects.\n",
        "\n",
        "# Raw data for Ticker\n",
        "if st.checkbox('Show raw data'):\n",
        "\n",
        "    if len(ticker_name) < 1:\n",
        "        st.warning(\"Please enter a ticker to analyze\")\n",
        "\n",
        "    if start_date < end_date:\n",
        "        st.success('Start date: `%s`\\n\\nEnd date: `%s`' % (start_date, end_date))\n",
        "    else:\n",
        "        st.error('Error: End date must fall after start date.')\n",
        "\n",
        "    st.subheader('Raw data from ' + str(start_date) + ' to ' + str(end_date))\n",
        "\n",
        "    # fetches the data: Open, Close, High, Low and Volume\n",
        "    if len(ticker_name) >= 1:\n",
        "      company = get_hist_data(ticker_name, start_date, end_date)\n",
        "\n",
        "      st.write(company)\n",
        "\n",
        "    # Add fundamental indicators from ticker.info\n",
        "\n",
        "    with st.form(\"form2\"):\n",
        "        if st.checkbox(\"Analyze fundamentals\"):\n",
        "\n",
        "            indicators = st.multiselect(\n",
        "            'Select from the following list',\n",
        "            ['ebitda', 'twoHundredDayAverage', ])\n",
        "\n",
        "            # This should be able to handle a list of indicators\n",
        "            for indicator in indicators:\n",
        "                \n",
        "                st.write(str(indicator))\n",
        "                st.write(ticker.info[indicator])\n",
        "\n",
        "        # News NOTE: GIVES ERRORS\n",
        "        if st.checkbox(\"Show \" + str(ticker.info['longName']) + \" related news (BETA)\"):\n",
        "            news = print_news(ticker)\n",
        "            st.write(news)\n",
        "\n",
        "        # analyst recommendations\n",
        "        if st.checkbox(\"Analyst recommendations\"):\n",
        "            recoms = get_recommendations(ticker)\n",
        "            st.write(recoms)\n",
        "\n",
        "        submitted = st.form_submit_button(\"Submit\")\n",
        "        if submitted:\n",
        "            st.success(\"Loading data!\")\n",
        " \n",
        "######## CHARTS ########\n",
        "\n",
        "# plots the graph\n",
        "if st.checkbox('Show charts for Start date: %s\\n\\nEnd date: %s' % (start_date, end_date)):\n",
        "\n",
        "    ts_start_date = st.date_input('Time series start date', start_date)\n",
        "    ts_end_date = st.date_input('Time series end date', end_date)\n",
        "\n",
        "    ts_stock_prices = get_hist_data(ticker_name, ts_start_date, ts_end_date)\n",
        "\n",
        "    plot_choice = st.selectbox(\"Choose a plot to display\", options = [\"Time Series\", \"Candlestick\", \"OHLC\"])\n",
        "\n",
        "    if plot_choice == \"Time Series\":\n",
        "\n",
        "        st.subheader('Time series plot for ' + str(ticker.info['longName']) + ' price')    \n",
        "        plot_time_series(ts_stock_prices)\n",
        "    \n",
        "    elif plot_choice == \"Candlestick\":\n",
        "\n",
        "        st.subheader('Candlestick chart for ' + str(ticker.info['longName']) + ' price')\n",
        "        make_candlestick(ts_stock_prices, ticker_name)\n",
        "\n",
        "    elif plot_choice == \"OHLC\":\n",
        "\n",
        "        st.subheader('OHLC chart for ' + str(ticker.info['longName']) + ' price')\n",
        "        make_ohlc(ts_stock_prices, ticker_name)\n",
        "  \n",
        "######## TWEETS ########\n",
        "\n",
        "# input bars to select ticker and num_tweets\n",
        "\n",
        "if st.checkbox(\"Build Sentiment Analysis Pipeline...\"):\n",
        "\n",
        "      rangeDays = (end_date - start_date).days\n",
        "\n",
        "      text = st.text_input('Enter text to analyze')\n",
        "\n",
        "      # Here we can have an input box for the user to select a model\n",
        "      model_name = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "      # Load models\n",
        "      with st.spinner(\"Loading...\"):\n",
        "\n",
        "          classifier = compute_sentiment(model_name)\n",
        "      \n",
        "      st.success(\"Ready!\")\n",
        "\n",
        "      st.write(\"Here are our predictions. NOTE: Daily sentimet is computed as a weighted sum of the number of positive and negative tweets.\" +\n",
        "      \"If the sum is positive then daily sentiment is considered overall positive, negative otherwise.\")\n",
        "\n",
        "      results = classifier(text)\n",
        "\n",
        "      labels = []\n",
        "      scores = []\n",
        "\n",
        "      for i in results:\n",
        "          label = i['label']\n",
        "          score = i['score']\n",
        "\n",
        "          if label == \"Neutral\": \n",
        "              labels.append(label+\" 0\")\n",
        "          elif label == \"Positive\":\n",
        "              labels.append(label+\" 1\")\n",
        "          else:\n",
        "              labels.append(label+\"-1\")     \n",
        "          \n",
        "          scores.append(score)\n",
        "\n",
        "      results_df = pd.DataFrame(results)\n",
        "\n",
        "      sent_scores = []\n",
        "\n",
        "      for i in range(len(labels)):\n",
        "          sent_score = float(labels[i][-2:]) * scores[i]\n",
        "          sent_scores.append(sent_score)\n",
        "\n",
        "      scores_df = pd.concat([pd.Series(text), results_df], axis=1)\n",
        "\n",
        "      scores_df[\"sent_score\"] = sent_scores\n",
        "\n",
        "      st.write(scores_df)\n",
        "\n",
        "      daily_avg_sentiment = ( sum(scores_df['sent_score']) / rangeDays )\n",
        "\n",
        "      ## Derive more efficient way of computing sentiment score ##\n",
        "      st.write(\"Daily Average Sentiment\")\n",
        "      st.write(daily_avg_sentiment)\n",
        "\n",
        "      if daily_avg_sentiment < 0:\n",
        "          st.write(emojis.encode(\"NEGATIVE :chart_with_downwards_trend:\"))\n",
        "\n",
        "      else:\n",
        "          st.write(emojis.encode(\"POSITIVE :chart_with_upwards_trend:\"))\n",
        "\n",
        "\n",
        "######### DATASET ##########\n",
        "\n",
        "if st.checkbox(\"Build and Display Dataset\"):\n",
        "\n",
        "    if len(ticker_name) < 1:\n",
        "        st.warning(\"Please enter a ticker to analyze\")\n",
        "\n",
        "    st.success('Loading full dataset for ' + str(ticker.info['longName']))\n",
        "\n",
        "    with st.spinner(\"Loading...\"):\n",
        "        # fetches the data: Open, Close, High, Low and Volume\n",
        "        companyDataset = get_hist_data(ticker_name, start_date, end_date, include_period=True)\n",
        "\n",
        "        date_df2 = companyDataset.reset_index()\n",
        "        firstDay2 = date_df2['Date'][0].date()\n",
        "        lastDay2 = date_df2['Date'][len(date_df2['Date'])-1].date()\n",
        "\n",
        "        start = time.time()\n",
        "        num_tweets2 = 2\n",
        "\n",
        "        rangeDays2 = (lastDay2 - firstDay2).days\n",
        "        st.write(\"You selected \", num_tweets2, \" tweets per day, for \", rangeDays2, \" days.\")\n",
        "\n",
        "        tweets_train = pd.DataFrame()\n",
        "        tweets_train_TEMP = pd.DataFrame()\n",
        "\n",
        "        for d in range(rangeDays2-1):\n",
        "\n",
        "            st.write(\"DAY: \", firstDay2)\n",
        "\n",
        "            dailyEnd2 = firstDay2 + datetime.timedelta(days=1)\n",
        "\n",
        "            tweets_with_date_day2 = get_tweets(ticker_name, num_tweets2, firstDay2.strftime(\"%Y-%m-%d %H:%M:%S\"), str(dailyEnd2))\n",
        "\n",
        "            tweets_day2 = tweetdf_to_list(tweets_with_date_day2)\n",
        "            \n",
        "            if len(tweets_day2) > 1:\n",
        "\n",
        "                cleanTweets_day2 = [preprocess(tweet) for tweet in tweets_day2]\n",
        "\n",
        "                tweets_train_TEMP['Date'] = tweets_with_date_day2['date']\n",
        "\n",
        "                tweets_train_TEMP['Tweet'] = pd.Series(cleanTweets_day2)\n",
        "\n",
        "                tweets_train_TEMP = tweetDates_to_DateTime(tweets_train_TEMP)\n",
        "\n",
        "                tweets_train = tweets_train.append(tweets_train_TEMP, ignore_index = True)\n",
        "                \n",
        "            firstDay2 += datetime.timedelta(days=1)\n",
        "\n",
        "        st.write(tweets_train['Tweet'].tolist())\n",
        "\n",
        "    model_name = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "    # Load models\n",
        "    with st.spinner(\"Loading...\"):\n",
        "\n",
        "        start1 = time.time()\n",
        "\n",
        "        classifier = compute_sentiment(model_name)\n",
        "    \n",
        "        st.success(\"Classifier ready! Building dataset...\")\n",
        "\n",
        "        results_train = classifier(tweets_train['Tweet'].tolist())\n",
        "\n",
        "        labels_train = []\n",
        "        scores_train = []\n",
        "\n",
        "        for i in results:\n",
        "            label = i['label']\n",
        "            score = i['score']\n",
        "\n",
        "            if label == \"Neutral\": \n",
        "                labels_train.append(label+\" 0\")\n",
        "            elif label == \"Positive\":\n",
        "                labels_train.append(label+\" 1\")\n",
        "            else:\n",
        "                labels_train.append(label+\"-1\")     \n",
        "            \n",
        "            scores_train.append(score)\n",
        "\n",
        "        results_train = pd.DataFrame(results_train)\n",
        "\n",
        "        sent_scores_train = []\n",
        "\n",
        "        for i in range(len(labels_train)):\n",
        "            sent_score = float(labels[i][-2:]) * scores[i]\n",
        "            sent_scores_train.append(sent_score)\n",
        "\n",
        "        scores_df_train = pd.concat([tweets_train, results_train], axis=1)\n",
        "\n",
        "        scores_df_train[\"sent_score\"] = sent_scores_train\n",
        "\n",
        "        st.write(scores_df_train)\n",
        "\n",
        "        #Group by date and sum sent scores\n",
        "        daily_sent_df = scores_df.groupby( [ pd.to_datetime( scores_df_train['Date'] ).dt.date ] )['sent_score'].sum()\n",
        "\n",
        "        daily_sent_df = pd.DataFrame(daily_sent_df)\n",
        "\n",
        "        #make date a column so we can edit it\n",
        "        companyDataset = companyDataset.reset_index()\n",
        "\n",
        "        #change date column (timestamp) to date\n",
        "        companyDataset = convert_ISO8601(companyDataset)\n",
        "\n",
        "        #set date back to index\n",
        "        companyDataset = companyDataset.set_index('Date')\n",
        "\n",
        "        #join historical data with daily sent avg data on index (date)\n",
        "        dataset = companyDataset.join(daily_sent_df)\n",
        "\n",
        "        end1 = time.time()\n",
        "\n",
        "    st.write(\"It took \", round(end1 - start1, 2), \" seconds to load models and build dataset of dimension \", dataset.shape )\n",
        "    st.write(dataset)\n",
        "\n",
        "    # Plot correlation matrix\n",
        "\n",
        "    st.subheader(\"Correlation Matrix\")\n",
        "    correlation_matrix(dataset)\n",
        "\n",
        "######### MODELS ##########\n",
        "\n",
        "# https://github.com/BenjiKCF/Neural-Net-with-Financial-Time-Series-Data/blob/master/5.%20Recurrent%20Neural%20Network.ipynb\n",
        "\n",
        "if st.checkbox(\"Machine Learning\"):\n",
        "\n",
        "  options = ['XGBoost', \"Recurrent Neural Network (RNN)\", \"Random Forest Classifier\", \"LSTM\"]\n",
        "\n",
        "  model_name = st.selectbox(\"Choose a model\", options)\n",
        "\n",
        "  st.write(\"You chose: \", model_name)\n",
        "\n",
        "  if st.checkbox(\"Build training dataset for \" + ticker_name):\n",
        "\n",
        "    from_date = st.date_input('From', today)\n",
        "    to_date = st.date_input('To', tomorrow)\n",
        "\n",
        "    if len(ticker_name) >= 1: \n",
        "      train_stock_prices = get_hist_data(ticker_name, from_date, to_date)\n",
        "\n",
        "    st.write(\"Here is your dataset for training:\")\n",
        "    st.write(train_stock_prices)\n",
        "\n",
        "    tech_indicator_options = ['RSI', 'SMA2 (rolling mean)', 'Force Index', \"MACD\", \"Aroon Oscillator\", \"On-Balance Volume (OBV)\"]\n",
        "    tech_indicators = st.multiselect(\"Select the technical indicators you would like to include in the model\", tech_indicator_options)\n",
        "\n",
        "  if st.checkbox(\"Load model\"):\n",
        "  \n",
        "    if model_name == \"Recurrent Neural Network (RNN)\":\n",
        "    \n",
        "      st.write(\"A recurrent neural network (RNN) is a type of artificial neural network which uses sequential data or time series data.\" + \n",
        "      \" These deep learning algorithms are commonly used for ordinal or temporal problems, such as language translation, natural language processing (nlp),\" + \n",
        "      \"  speech recognition, and image captioning; they are incorporated into popular applications such as Siri, voice search, and Google Translate.\" + \n",
        "      \" Like feedforward and convolutional neural networks (CNNs), recurrent neural networks utilize training data to learn.\" +\n",
        "      \" They are distinguished by their “memory” as they take information from prior inputs to influence the current input and output.\" + \n",
        "      \" While traditional deep neural networks assume that inputs and outputs are independent of each other, the output of recurrent neural networks\" + \n",
        "      \" depend on the prior elements within the sequence. While future events would also be helpful in determining the output of a given sequence, unidirectional\" + \n",
        "      \" recurrent neural networks cannot account for these events in their predictions.\")\n",
        "\n",
        "      # Load model\n",
        "      df = train_stock_prices\n",
        "      df[\"Diff\"] = df.Close.diff()\n",
        "      df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
        "\n",
        "      df = append_TI(df, tech_indicators)\n",
        "\n",
        "      df = df.drop(\n",
        "          [ \"Diff\"],\n",
        "          axis=1,\n",
        "      ).dropna()\n",
        "\n",
        "\n",
        "      X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
        "      y = df[\"y\"].values\n",
        "      X_train, X_test, y_train, y_test = train_test_split(\n",
        "          X,\n",
        "          y,\n",
        "          test_size=0.2,\n",
        "          shuffle=False,\n",
        "      )\n",
        "      model = Sequential()\n",
        "      model.add(SimpleRNN(2, input_shape=(X_train.shape[1], 1)))\n",
        "      model.add(Dense(1, activation=\"sigmoid\"))\n",
        "      model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "      history = model.fit(X_train[:, :, np.newaxis], y_train, validation_split=0.2, epochs=100)\n",
        "      y_pred = model.predict(X_test[:, :, np.newaxis])\n",
        "\n",
        "\n",
        "      st.success(\"Success!\")\n",
        "      st.write(df)\n",
        "\n",
        "      with st.form(\"form\"):\n",
        "        acc_checkbox = st.checkbox(\"Test Simple RNN Accuracy\")\n",
        "        if acc_checkbox:\n",
        "          st.write(\"Model accuracy on test dataset: \", accuracy_score(y_test, y_pred > 0.5))\n",
        "\n",
        "        loss_checkbox = st.checkbox(\"View Loss Plot\")\n",
        "        if loss_checkbox:\n",
        "          plt.plot(history.history['loss'])\n",
        "          plt.plot(history.history['val_loss'])\n",
        "          plt.title('model loss')\n",
        "          plt.ylabel('loss')\n",
        "          plt.xlabel('epoch')\n",
        "          plt.legend(['train', 'test'], loc='upper left')\n",
        "          st.pyplot(plt)\n",
        "          #st.write(history.history['acc'])\n",
        "\n",
        "        visual_checkbox = st.checkbox(\"Visualize RNN\")\n",
        "        if visual_checkbox:\n",
        "          plot_model(model, to_file='rnn_model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "          image = Image.open('rnn_model_plot.png')\n",
        "          st.image(image)\n",
        "\n",
        "        pred_checkbox = st.checkbox(\"Predict Tomorrows Movement\")\n",
        "        if pred_checkbox:\n",
        "          y_pred = model.predict(X_test[:, :, np.newaxis][-1].reshape(1, -1))\n",
        "          st.write(y_pred)\n",
        "          #y_pred = model.predict(X_test[-1].reshape(1, -1))\n",
        "\n",
        "          #if y_pred == 1:\n",
        "           # st.write(\"The model predicts \" + ticker_name + \" stock will increase tomorrow\")\n",
        "\n",
        "          #else:\n",
        "           # st.write(\"The model predicts \" + ticker_name + \" stock will decrease tomorrow\")\n",
        "\n",
        "        submitted = st.form_submit_button(\"Submit\")\n",
        "        if submitted:\n",
        "          st.write(\"checkbox\", loss_checkbox, \"checkbox\", acc_checkbox, \"checkbox\", visual_checkbox)\n",
        "\n",
        "      # Display predictions and Cost history\n",
        "\n",
        "\n",
        "    if model_name == 'XGBoost':\n",
        "\n",
        "      st.write(\"As the name may reveal, XGBoost the Algorithm is a gradient boosting algorithm, a common technique in ensemble learning.\"+\n",
        "              \" To unpack that new phrase, ensemble learning is a type of machine learning that enlists many models to make predictions together.\"+ \n",
        "              \" Boosting algorithms are distinguished from other ensemble learning techniques by building a sequence of initially weak models into increasingly more powerful models.\"+ \n",
        "              \" Gradient boosting algorithms choose how to build a more powerful model using the gradient of a loss function that captures the performance of a model.\"+\n",
        "              \" Gradient boosting is a foundational approach to many machine learning algorithms.\"+\n",
        "              \" XGBoost has solidified its name in the boosting game with its use in many competition-winning models and prolific reference in research.\")\n",
        "\n",
        "      \n",
        "      # Load model\n",
        "\n",
        "\n",
        "      # Data Pre Processing\n",
        "\n",
        "\n",
        "      # Train models\n",
        "\n",
        "\n",
        "      # Display predictions and Cost history\n",
        "\n",
        "\n",
        "    if model_name == 'Random Forest Classifier':\n",
        "\n",
        "      st.write(\"\"\"Random Forest (RF) is an ensemble learning procedure for regression and classification. As with XGBoost, RF employs many models to make predictions together.\n",
        "                  The models utilized in RF are decision trees, and is implemented as an ensemble in order to reduce variance. Decision tress can have a tendency to run deep and overfit models,\n",
        "                  thus random forests work well to average a multitude of decision trees and achieve an ideal result. RF is a great model for stock prediction, as it handles large datasets\n",
        "                  well and predicts with relative speed. Hyperparameter tuning must be done carefully, as deep RF models can be difficult to interpret. \"\"\")\n",
        "\n",
        "      \n",
        "      # Load model\n",
        "      df = train_stock_prices\n",
        "      df[\"Diff\"] = df.Close.diff()\n",
        "      df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
        "\n",
        "      df = append_TI(df, tech_indicators)\n",
        "            \n",
        "        \n",
        "      df = df.drop(\n",
        "        [\"Diff\"],\n",
        "        axis=1,\n",
        "      ).dropna()\n",
        "      X = df.drop([\"y\"], axis=1).values\n",
        "      y = df[\"y\"].values\n",
        "      X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.2,\n",
        "        shuffle=False,\n",
        "      )\n",
        "\n",
        "      @st.cache\n",
        "      def Classify_RF(df):\n",
        "        RF_Model = RandomForestClassifier()\n",
        "        RF_Model.fit(\n",
        "          X_train,\n",
        "          y_train,\n",
        "        )\n",
        "\n",
        "        return RF_Model\n",
        "\n",
        "      @st.cache\n",
        "      def RF_Predictions(RF_Model):\n",
        "        y_pred = RF_Model.predict(X_test)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "      RF_Model = Classify_RF(df)\n",
        "      y_pred = RF_Predictions(RF_Model)\n",
        "      \n",
        "\n",
        "      st.success(\"Success!\")\n",
        "      st.write(df)\n",
        "  \n",
        "      with st.form(\"form\"):\n",
        "        feat_check = st.checkbox(\"See Feature Importance\")\n",
        "        if feat_check:\n",
        "\n",
        "          features = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n",
        "\n",
        "          for indicator in tech_indicators:\n",
        "            features.append(indicator)\n",
        "\n",
        "          fig = go.Figure([go.Bar(x = features, y = RF_Model.feature_importances_)])\n",
        "          fig.update_layout(title= 'Feature Importances', xaxis_title = 'Features')\n",
        "          st.write(fig)\n",
        "\n",
        "        acc_checkbox = st.checkbox(\"Test Random Forest Classifier Accuracy\")\n",
        "        if acc_checkbox:\n",
        "          st.write(\"Model accuracy on test dataset: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "        if st.checkbox(\"View Confusion Matrix\"):\n",
        "          st.write(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "        report_checkbox = st.checkbox(\"View Classification Report\")\n",
        "        if report_checkbox:\n",
        "          report = classification_report(y_test, y_pred, output_dict=True)\n",
        "          report = pd.DataFrame(report).transpose()\n",
        "          st.write(report)\n",
        "\n",
        "        pred_checkbox = st.checkbox(\"Predict Tomorrows Movement\")\n",
        "        if pred_checkbox:\n",
        "          y_pred = RF_Model.predict(X_test[-1].reshape(1, -1))\n",
        "\n",
        "          if y_pred == 1:\n",
        "            st.write(\"The model predicts \" + ticker_name + \" stock will increase tomorrow\")\n",
        "\n",
        "          else:\n",
        "            st.write(\"The model predicts \" + ticker_name + \" stock will decrease tomorrow\")\n",
        "\n",
        "\n",
        "        submitted = st.form_submit_button(\"Submit\")\n",
        "        if submitted:\n",
        "          st.write(\"checkbox\", feat_check, \"checkbox\", acc_checkbox, \"checkbox\", report_checkbox, \"checkbox\", pred_checkbox)\n",
        "\n",
        "\n",
        "      # Data Pre Processing\n",
        "\n",
        "\n",
        "      # Train models\n",
        "\n",
        "\n",
        "      # Display predictions and Cost history\n",
        "\n",
        "    if model_name == 'LSTM':\n",
        "    \n",
        "      df = train_stock_prices\n",
        "      df[\"Diff\"] = df.Close.diff()\n",
        "      df[\"y\"] = df[\"Diff\"].apply(lambda x: 1 if x > 0 else 0).shift(-1)\n",
        "\n",
        "      df = append_TI(df, tech_indicators)\n",
        "      \n",
        "      df = df.drop(\n",
        "        [\"Diff\"],\n",
        "        axis=1,\n",
        "      ).dropna()\n",
        "\n",
        "      X = StandardScaler().fit_transform(df.drop([\"y\"], axis=1))\n",
        "      y = df[\"y\"].values\n",
        "      X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        test_size=0.2,\n",
        "        shuffle=False,\n",
        "      )\n",
        "      model = Sequential()\n",
        "      model.add(LSTM(2, input_shape=(X_train.shape[1], 1)))\n",
        "      model.add(Dense(1, activation=\"sigmoid\"))\n",
        "      model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "      history = model.fit(X_train[:, :, np.newaxis], y_train, validation_split=0.2, epochs=100)\n",
        "      y_pred = model.predict(X_test[:, :, np.newaxis])\n",
        "      \n",
        "      st.success(\"Success!\")\n",
        "      st.write(df)\n",
        "\n",
        "      with st.form(\"form\"):\n",
        "\n",
        "        acc_checkbox = st.checkbox(\"Test LSTM Accuracy\")\n",
        "        if acc_checkbox:\n",
        "          st.write(\"Model accuracy on test dataset: \", accuracy_score(y_test, y_pred > 0.5))\n",
        "\n",
        "        loss_checkbox = st.checkbox(\"View Loss Plot\")\n",
        "        if loss_checkbox:\n",
        "          plt.plot(history.history['loss'])\n",
        "          plt.plot(history.history['val_loss'])\n",
        "          plt.title('model loss')\n",
        "          plt.ylabel('loss')\n",
        "          plt.xlabel('epoch')\n",
        "          plt.legend(['train', 'test'], loc='upper left')\n",
        "          st.pyplot(plt)\n",
        "          #st.write(history.history['acc'])\n",
        "\n",
        "        visual_checkbox = st.checkbox(\"Visualize LSTM Network\")\n",
        "        if visual_checkbox:\n",
        "          plot_model(model, to_file='lstm_model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "          image = Image.open('lstm_model_plot.png')\n",
        "          st.image(image)\n",
        "\n",
        "        submitted = st.form_submit_button(\"Submit\")\n",
        "        if submitted:\n",
        "          st.write(\"checkbox\", acc_checkbox, \"checkbox\", loss_checkbox, \"checkbox\", visual_checkbox)\n",
        "\n",
        "      # As of now, Keras doesn't provide a way to extract feature importance. This is the closest I have come but\n",
        "      # It does not work with sequential models\n",
        "      if st.checkbox(\"Show Feature Importances\"):\n",
        "        import shap\n",
        "        shap.initjs()\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_values = explainer.shap_values(X_train[:, :, np.newaxis])\n",
        "\n",
        "        # visualize the first prediction's explanation \n",
        "        shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:], matplotlib = True)\n",
        "\n",
        "        st.pyplot(shap.summary_plot(shap_values, X_train[:, :, np.newaxis], plot_type=\"bar\"))\n",
        "\n",
        "######### PORTFOLIO OPTIMIZATION ##########\n",
        "\n",
        "if st.checkbox(\"Run Monte Carlo Portfolio Optimization algorithm\"):\n",
        "\n",
        "    tickerOptions = ['AAPL', 'ROKU', 'MGNI', 'TSLA', 'AMD', 'PFE', 'MRNA', 'JNJ', \"GOOGL\",\"FB\", \"JPM\", \"BAC\", \"HSBC\"]\n",
        "\n",
        "    tickers = st.multiselect(\"Select the tickers you would like to consider in your portfolio\", tickerOptions)\n",
        "\n",
        "    # run optimization algorithm\n",
        "\n",
        "    start_dateOP = st.date_input('Analyze data from', today)\n",
        "    end_dateOP = st.date_input('Analyze data to', tomorrow)\n",
        "\n",
        "    prices_df = pd.DataFrame()\n",
        "\n",
        "    if st.checkbox(\"Run!\"):\n",
        "\n",
        "        for ticker in tickers:\n",
        "            \n",
        "            data = get_hist_data(ticker, start_dateOP, end_dateOP)\n",
        "\n",
        "            data_date = data.reset_index()\n",
        "\n",
        "            data_date[f\"{ticker}\"] = data_date['Adj Close']\n",
        "\n",
        "            data_date = data_date.set_index('Date')\n",
        "            \n",
        "            if len(prices_df) < 1:\n",
        "                prices_df = pd.concat([prices_df, data_date[f\"{ticker}\"]], axis=1)\n",
        "            \n",
        "            else:\n",
        "                prices_df = prices_df.join(data_date[f\"{ticker}\"])\n",
        "\n",
        "        st.write(prices_df)\n",
        "        # \"Optimize with Mean-Variance Optimization\", \n",
        "        option = st.selectbox(\"Choose model\", [\"Monte Carlo Simulation\"])\n",
        "\n",
        "        # if option == \"Optimize with Mean-Variance Optimization\":\n",
        "\n",
        "        #     # MAKE THIS USER INTERACTIVE\n",
        "        #     mu = ema_historical_return(prices_df)\n",
        "        #     S = exp_cov(prices_df)\n",
        "        #     ef = EfficientFrontier(mu, S, solver=\"SCS\")\n",
        "\n",
        "        #     weights = ef.min_volatility()\n",
        "        #     cleaned_weights = ef.clean_weights()\n",
        "        #     port_performance = ef.portfolio_performance(verbose=True)\n",
        "        #     st.write(port_performance)\n",
        "\n",
        "        #     latest_prices = get_latest_prices(prices_df)\n",
        "        #     da = DiscreteAllocation(weights, latest_prices, total_portfolio_value=100000)\n",
        "        #     allocation, leftover = da.greedy_portfolio()\n",
        "        #     st.write(\"Discrete allocation:\", allocation)\n",
        "        #     st.write(\"Funds remaining: ${:.2f}\".format(leftover))\n",
        "\n",
        "        if option == \"Monte Carlo Simulation\":\n",
        "\n",
        "            np.random.seed(42) # for replicability\n",
        "\n",
        "            returns_portfolio = prices_df.pct_change()\n",
        "            st.write(\"Returns\", returns_portfolio)\n",
        "            cov = returns_portfolio.cov()\n",
        "\n",
        "            portfolios = pd.DataFrame(columns=[*tickers, \"Expected Return\",\"Portfolio Variance\", \"Portfolio Std\", \"Sharpe Ratio\"])\n",
        "\n",
        "            port_returns = []\n",
        "            port_volatility = []\n",
        "            port_weights = []\n",
        "\n",
        "            num_assets = len(prices_df.columns)\n",
        "            num_portfolios = st.number_input(\"Insert number of portfolios to simulate\", min_value=100, max_value=1000000)\n",
        "\n",
        "            #make date a column so we can edit it\n",
        "            prices_df = prices_df.reset_index()\n",
        "\n",
        "            #change date column (timestamp) to date\n",
        "            prices_df['index'] = pd.to_datetime(prices_df['index'])\n",
        "\n",
        "            #set date back to index\n",
        "            prices_df = prices_df.set_index('index')\n",
        "\n",
        "            daily_indv_returns = prices_df.pct_change()\n",
        "            daily_indv_returns = daily_indv_returns.dropna()\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            for port in range(int(num_portfolios)):\n",
        "\n",
        "                weights = np.random.random(num_assets)\n",
        "                weights = weights / np.sum(weights) # Sum must equal 1\n",
        "\n",
        "                portfolios.loc[port, tickers] = weights\n",
        "                metrics = portfolio_metrics(returns_portfolio, weights, 0.02, port)\n",
        "                portfolios.loc[port, [\"Expected Return\",\"Portfolio Variance\", \"Portfolio Std\", \"Sharpe Ratio\"]] = \\\n",
        "                metrics.loc[port,[\"Expected Return\",\"Portfolio Variance\", \"Portfolio Std\", \"Sharpe Ratio\"]]\n",
        "            \n",
        "            end_time = time.time()\n",
        "\n",
        "            st.write(\"It took \", round(end_time - start_time, 2), \" seconds to simulate \", num_portfolios, \" for \", (end_dateOP - start_dateOP).days, \" days.\")\n",
        "            st.write(portfolios)\n",
        "\n",
        "            # plot efficient frontier\n",
        "            make_efficient_frontier(portfolios[\"Portfolio Variance\"], portfolios[\"Expected Return\"])\n",
        "\n",
        "            if st.checkbox(\"Minimize volatility\"):\n",
        "\n",
        "                # Use new formula for better results\n",
        "                optimal_min_portfolio = portfolios[portfolios[\"Portfolio Variance\"]==portfolios[\"Portfolio Variance\"].min()].T\n",
        "                opt_min_weights = portfolios[portfolios[\"Portfolio Variance\"]==portfolios[\"Portfolio Variance\"].min()].to_numpy()[0][0:len(tickers)]\n",
        "                min_allocation = optimal_portfolio_weights(opt_min_weights, tickers)\n",
        "                st.write(\"Reccomended allocation for minimum variance\", min_allocation)\n",
        "\n",
        "                make_efficient_frontier_opt(portfolios[\"Portfolio Variance\"], portfolios[\"Expected Return\"], (optimal_min_portfolio.T)[\"Portfolio Variance\"].values[0], (optimal_min_portfolio.T)[\"Expected Return\"].values[0], 0, 0)\n",
        "\n",
        "            if st.checkbox(\"Maximize Sharpe Ratio\"):\n",
        "\n",
        "                risk_factor = 0.01 # it is usually the treasury bond yield\n",
        "\n",
        "                # Use new formula for better results\n",
        "                optimal_risk_port = portfolios[portfolios[\"Sharpe Ratio\"]==portfolios[\"Sharpe Ratio\"].max()].T\n",
        "                #optimal_risk_port_df = optimal_risk_port.to_frame()\n",
        "                optimal_risk_port.columns = ['Summary']\n",
        "                \n",
        "\n",
        "                opt_weights = portfolios[portfolios[\"Sharpe Ratio\"]==portfolios[\"Sharpe Ratio\"].max()].to_numpy()[0][0:len(tickers)]\n",
        "                allocation = optimal_portfolio_weights(opt_weights, tickers)\n",
        "                st.write(\"Reccomended allocation for maximum Sharpe Ratio\", allocation)\n",
        "\n",
        "                make_efficient_frontier_opt(portfolios[\"Portfolio Variance\"], portfolios[\"Expected Return\"], 0, 0, (optimal_risk_port.T)[\"Portfolio Variance\"][0], (optimal_risk_port.T)[\"Expected Return\"][0], min_volatility = False)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "py2nao-ZA_IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJbmhCm3lr8Z"
      },
      "source": [
        "## Server set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E3uSaXvlYj0",
        "outputId": "79031187-c6d6-4218-84f8-c067ace3321d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "## Authtoken needed to connect to server\n",
        "\n",
        "!ngrok authtoken 267bF5z4JLHYkOC3tXZj4e2GYW5_7hYxKXE8fRwrYw1niAjUE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Often, after running ngrok.connect(...) the tunnel fails to connect. If this happens simply close the window, run the cell ngrok.kill() and re run ngrok.connect(...). The issue will be fixed."
      ],
      "metadata": {
        "id": "LmX7Jn4vpvny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8RBLwlYHnQ49"
      },
      "outputs": [],
      "source": [
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y331Eg1dglo7",
        "outputId": "b4c53f2a-7f3f-4b94-d04e-0e86ee8adb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "http://00f8-35-245-211-201.ngrok.io\n"
          ]
        }
      ],
      "source": [
        "!nohup streamlit run app.py &\n",
        "url = ngrok.connect(port = '8501')\n",
        "print(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAxTDkOzndBH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "StockPredDemo_v.6.0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}